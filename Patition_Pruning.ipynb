{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvY/2nCkcAAeW5u16tvUen"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezzn1vUeVPL_"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"Pruning\").getOrCreate()"
      ],
      "metadata": {
        "id": "PrV9EZSzWHBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Sample Data\n",
        "\n",
        "data = [\n",
        "    (\"Alice\", \"HR\", 1000),\n",
        "    (\"Bob\", \"IT\", 1500),\n",
        "    (\"Charlie\", \"HR\", 1200),\n",
        "    (\"David\", \"IT\", 1800),\n",
        "    (\"Eve\", \"Finance\", 2000),\n",
        "    (\"Frank\", \"Finance\", 1800)\n",
        "]\n",
        "\n",
        "columns = [\"employee_name\", \"department\", \"salary\"]"
      ],
      "metadata": {
        "id": "Ztx6-An5VZ3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating Data Frame\n",
        "df = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "e61E8BCJV5Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Write DataFrame using partitioning\n",
        "output_path = \"/content/sample_data/OutputData\"\n",
        "\n",
        "df.write\\\n",
        "  .mode(\"overwrite\")\\\n",
        "  .partitionBy(\"department\")\\\n",
        "  .parquet(output_path)"
      ],
      "metadata": {
        "id": "ZGgrTad8WQTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Write DataFrame using without partitioning\n",
        "output_path_new = \"/content/sample_data/OutputDataWithout\"\n",
        "\n",
        "df.write\\\n",
        "  .mode(\"overwrite\")\\\n",
        "  .parquet(output_path_new)"
      ],
      "metadata": {
        "id": "rJVd5kDQY5M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_withpart = spark.read.format(\"parquet\").load(output_path)"
      ],
      "metadata": {
        "id": "9Hw1oUHyZfTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_withpart.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhMZ1-VAZqo1",
        "outputId": "ba801b9f-7733-45d0-9461-c1c09821b3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------+----------+\n",
            "|employee_name|salary|department|\n",
            "+-------------+------+----------+\n",
            "|        Alice|  1000|        HR|\n",
            "|      Charlie|  1200|        HR|\n",
            "|        David|  1800|        IT|\n",
            "|          Eve|  2000|   Finance|\n",
            "|        Frank|  1800|   Finance|\n",
            "|          Bob|  1500|        IT|\n",
            "+-------------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_withpart.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whiiJ5P1bAeX",
        "outputId": "130fcd97-ed36-441f-d215-12d6fe537abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "Relation [employee_name#132,salary#133L,department#134] parquet\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "employee_name: string, salary: bigint, department: string\n",
            "Relation [employee_name#132,salary#133L,department#134] parquet\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Relation [employee_name#132,salary#133L,department#134] parquet\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) ColumnarToRow\n",
            "+- FileScan parquet [employee_name#132,salary#133L,department#134] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/sample_data/OutputData], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<employee_name:string,salary:bigint>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "qeBPdJzTaosV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_withpart = spark.read.format(\"parquet\").load(output_path)"
      ],
      "metadata": {
        "id": "W_VGo2vOavX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_withpart.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReI5KpYCaznH",
        "outputId": "7a37596e-d4b4-46c0-fedc-d5f059cca11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------+----------+\n",
            "|employee_name|salary|department|\n",
            "+-------------+------+----------+\n",
            "|        Alice|  1000|        HR|\n",
            "|      Charlie|  1200|        HR|\n",
            "+-------------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_withpart.explain(True)"
      ],
      "metadata": {
        "id": "rIvURHzSbOXq",
        "outputId": "8aa7dabb-37b2-4840-9b21-73be2132dbc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Filter ('department = HR)\n",
            "+- Relation [employee_name#152,salary#153L,department#154] parquet\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "employee_name: string, salary: bigint, department: string\n",
            "Filter (department#154 = HR)\n",
            "+- Relation [employee_name#152,salary#153L,department#154] parquet\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Filter (isnotnull(department#154) AND (department#154 = HR))\n",
            "+- Relation [employee_name#152,salary#153L,department#154] parquet\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) ColumnarToRow\n",
            "+- FileScan parquet [employee_name#152,salary#153L,department#154] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/sample_data/OutputData], PartitionFilters: [isnotnull(department#154), (department#154 = HR)], PushedFilters: [], ReadSchema: struct<employee_name:string,salary:bigint>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_withoutpart = spark.read.format(\"parquet\").load(output_path_new).filter(col(\"department\") == \"HR\")"
      ],
      "metadata": {
        "id": "ghf93LLfftG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_withoutpart.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6wJcEzIg5-v",
        "outputId": "9b7ac1f2-2b40-40d3-e0ec-edefdf8273b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|        Alice|        HR|  1000|\n",
            "|      Charlie|        HR|  1200|\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_withpart.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogUw7axShIpi",
        "outputId": "c5198589-9699-4de9-c982-10f967a3d354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------+----------+\n",
            "|employee_name|salary|department|\n",
            "+-------------+------+----------+\n",
            "|        Alice|  1000|        HR|\n",
            "|      Charlie|  1200|        HR|\n",
            "|        David|  1800|        IT|\n",
            "|          Eve|  2000|   Finance|\n",
            "|        Frank|  1800|   Finance|\n",
            "|          Bob|  1500|        IT|\n",
            "+-------------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dynamic Partition Pruning**"
      ],
      "metadata": {
        "id": "kizFIlOVgsu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_join = df_withoutpart.join(df_withpart,\n",
        "                              (df_withoutpart['employee_name'] == df_withpart['employee_name']) &\n",
        "                               (df_withoutpart['department']== df_withpart['department']), \"inner\")"
      ],
      "metadata": {
        "id": "Fb1J62DQfzVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_join.show()"
      ],
      "metadata": {
        "id": "tJ9xioiFhdA3",
        "outputId": "2fe42362-4e13-4591-eda9-0dad41e2538b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+-------------+------+----------+\n",
            "|employee_name|department|salary|employee_name|salary|department|\n",
            "+-------------+----------+------+-------------+------+----------+\n",
            "|        Alice|        HR|  1000|        Alice|  1000|        HR|\n",
            "|      Charlie|        HR|  1200|      Charlie|  1200|        HR|\n",
            "+-------------+----------+------+-------------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_join.explain(True)"
      ],
      "metadata": {
        "id": "7H9_67hahfw8",
        "outputId": "e08d4184-8b80-44f9-e8d2-ecf8be831d9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "Join Inner, ((employee_name#175 = employee_name#195) AND (department#176 = department#197))\n",
            ":- Filter (department#176 = HR)\n",
            ":  +- Relation [employee_name#175,department#176,salary#177L] parquet\n",
            "+- Relation [employee_name#195,salary#196L,department#197] parquet\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "employee_name: string, department: string, salary: bigint, employee_name: string, salary: bigint, department: string\n",
            "Join Inner, ((employee_name#175 = employee_name#195) AND (department#176 = department#197))\n",
            ":- Filter (department#176 = HR)\n",
            ":  +- Relation [employee_name#175,department#176,salary#177L] parquet\n",
            "+- Relation [employee_name#195,salary#196L,department#197] parquet\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Join Inner, ((employee_name#175 = employee_name#195) AND (department#176 = department#197))\n",
            ":- Filter ((isnotnull(department#176) AND (department#176 = HR)) AND isnotnull(employee_name#175))\n",
            ":  +- Relation [employee_name#175,department#176,salary#177L] parquet\n",
            "+- Filter (((department#197 = HR) AND isnotnull(employee_name#195)) AND isnotnull(department#197))\n",
            "   +- Relation [employee_name#195,salary#196L,department#197] parquet\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- BroadcastHashJoin [employee_name#175, department#176], [employee_name#195, department#197], Inner, BuildLeft, false\n",
            "   :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false], input[1, string, false]),false), [plan_id=376]\n",
            "   :  +- Filter ((isnotnull(department#176) AND (department#176 = HR)) AND isnotnull(employee_name#175))\n",
            "   :     +- FileScan parquet [employee_name#175,department#176,salary#177L] Batched: true, DataFilters: [isnotnull(department#176), (department#176 = HR), isnotnull(employee_name#175)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/sample_data/OutputDataWithout], PartitionFilters: [], PushedFilters: [IsNotNull(department), EqualTo(department,HR), IsNotNull(employee_name)], ReadSchema: struct<employee_name:string,department:string,salary:bigint>\n",
            "   +- Filter isnotnull(employee_name#195)\n",
            "      +- FileScan parquet [employee_name#195,salary#196L,department#197] Batched: true, DataFilters: [isnotnull(employee_name#195)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/sample_data/OutputData], PartitionFilters: [(department#197 = HR), isnotnull(department#197)], PushedFilters: [IsNotNull(employee_name)], ReadSchema: struct<employee_name:string,salary:bigint>\n",
            "\n"
          ]
        }
      ]
    }
  ]
}