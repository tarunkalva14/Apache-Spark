{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfbcmouu2lnPc0EaxDaUSI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g5RymGnRllU3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('AQE').getOrCreate()"
      ],
      "metadata": {
        "id": "_a5a7a_AmaX4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "nHWmiYw4mc9E",
        "outputId": "e628b956-7de2-48dd-de6c-52f1ea3a4ab6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c5a793387d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://d9da63bb9217:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>AQE</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.adaptive.enabled\",True)"
      ],
      "metadata": {
        "id": "M7lfxVmamhpk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"Alice\", \"HR\", 10000),\n",
        "    (\"Bob\", \"IT\", 12000),\n",
        "    (\"Charlie\", \"Finance\", 9000),\n",
        "    (\"David\", \"HR\", 11000),\n",
        "    (\"Eve\", \"IT\", 13000),\n",
        "    (\"Frank\", \"Finance\", 8000)\n",
        "]\n",
        "columns = [\"employee_name\", \"department\", \"salary\"]\n",
        "df = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "AwIrF-H0m2DR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO4VM_h6okFu",
        "outputId": "951490ee-5faf-434e-bacd-9f1c6ba563b7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|        Alice|        HR| 10000|\n",
            "|          Bob|        IT| 12000|\n",
            "|      Charlie|   Finance|  9000|\n",
            "|        David|        HR| 11000|\n",
            "|          Eve|        IT| 13000|\n",
            "|        Frank|   Finance|  8000|\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "PFIt727upOcb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df =df.groupBy(\"department\").agg(count(\"employee_name\"))"
      ],
      "metadata": {
        "id": "OqWYgXOcopib"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiYfAQ5vpXX-",
        "outputId": "f1c57bf8-b5f6-4f7c-c7bc-448e45f307b5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[department#38], functions=[count(employee_name#37)])\n",
            "   +- Exchange hashpartitioning(department#38, 200), ENSURE_REQUIREMENTS, [plan_id=72]\n",
            "      +- HashAggregate(keys=[department#38], functions=[partial_count(employee_name#37)])\n",
            "         +- Project [employee_name#37, department#38]\n",
            "            +- Scan ExistingRDD[employee_name#37,department#38,salary#39L]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AQE JOIN Stratgy**"
      ],
      "metadata": {
        "id": "kg5OFM58rH1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d1 =[\n",
        "    (1,\"Alice\"),\n",
        "    (2,\"Bob\"),\n",
        "    (3,\"Charlie\"),\n",
        "    (4,\"David\"),\n",
        "    (5,\"Eve\"),\n",
        "    (6,\"Frank\")\n",
        "]\n",
        "df1 = spark.createDataFrame(d1,[\"id\",\"name\"])\n",
        "d2 = [\n",
        "    (1,100000),\n",
        "    (2,50000),\n",
        "    (3,90000),\n",
        "    (4,40000),\n",
        "    (5,80000),\n",
        "    (6,70000)\n",
        "]\n",
        "df2 = spark.createDataFrame(d2,[\"id\",\"salary\"])"
      ],
      "metadata": {
        "id": "3KF0BUaHrT_M"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = df1.join(df2,df1['id'] == df2['id'],\"left\")"
      ],
      "metadata": {
        "id": "3gNMje3dr-gm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMqlE0SvsImV",
        "outputId": "6febfc9a-be82-41ea-b10f-7b4206b6ad8e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+------+\n",
            "| id|   name| id|salary|\n",
            "+---+-------+---+------+\n",
            "|  1|  Alice|  1|100000|\n",
            "|  3|Charlie|  3| 90000|\n",
            "|  2|    Bob|  2| 50000|\n",
            "|  6|  Frank|  6| 70000|\n",
            "|  5|    Eve|  5| 80000|\n",
            "|  4|  David|  4| 40000|\n",
            "+---+-------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ASaiMxEskNK",
        "outputId": "d0c2a788-8ec2-424d-9b58-380aca32942a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "Join LeftOuter, (id#65L = id#69L)\n",
            ":- LogicalRDD [id#65L, name#66], false\n",
            "+- LogicalRDD [id#69L, salary#70L], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "id: bigint, name: string, id: bigint, salary: bigint\n",
            "Join LeftOuter, (id#65L = id#69L)\n",
            ":- LogicalRDD [id#65L, name#66], false\n",
            "+- LogicalRDD [id#69L, salary#70L], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Join LeftOuter, (id#65L = id#69L)\n",
            ":- LogicalRDD [id#65L, name#66], false\n",
            "+- Filter isnotnull(id#69L)\n",
            "   +- LogicalRDD [id#69L, salary#70L], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- SortMergeJoin [id#65L], [id#69L], LeftOuter\n",
            "   :- Sort [id#65L ASC NULLS FIRST], false, 0\n",
            "   :  +- Exchange hashpartitioning(id#65L, 200), ENSURE_REQUIREMENTS, [plan_id=219]\n",
            "   :     +- Scan ExistingRDD[id#65L,name#66]\n",
            "   +- Sort [id#69L ASC NULLS FIRST], false, 0\n",
            "      +- Exchange hashpartitioning(id#69L, 200), ENSURE_REQUIREMENTS, [plan_id=220]\n",
            "         +- Filter isnotnull(id#69L)\n",
            "            +- Scan ExistingRDD[id#69L,salary#70L]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}